{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing and evaluating numerous machine learning techniques to determine best option for predicting fruad occurances in Enron email dataset.  The most efficient predictor ended up being an Adaboost algorithm with 50 n_estimators.  This method using decision tree as a 'weak learner' came out with about 85% accuracy, p-value of 39, and an r-squared of around 32.  Originally conducted for Udacity Nanodegree project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named scipy",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1fc54d521c95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfeature_format\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeatureFormat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargetFeatureSplit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtester\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtest_classifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdump_classifier_and_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\blog\\content\\data\\tester.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStratifiedShuffleSplit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../tools/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfeature_format\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeatureFormat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargetFeatureSplit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\blog\\dsblog\\lib\\site-packages\\sklearn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m     \u001b[0m__check_build\u001b[0m  \u001b[1;31m# avoid flakes unused variable error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\blog\\dsblog\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named scipy"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import numpy\n",
    "sys.path.append(\"../data/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import test_classifier, dump_classifier_and_data\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import *\n",
    "\n",
    "################################\n",
    "# feature selection\n",
    "################################\n",
    "features_list = ['poi','salary', 'bonus', 'exercised_stock_options', 'to_poi_percentage',\n",
    "                 'long_term_incentive', 'expenses', 'director_fees', 'total_payments']\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "data_dict = pickle.load(open(\"../data/final_project_dataset.pkl\", \"r\") )\n",
    "#n of data points\n",
    "print len(data_dict)\n",
    "\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "data_dict.pop('TOTAL',0)  #first outlier..  invalid spreadsheet input\n",
    "my_dataset = data_dict\n",
    "\n",
    "################################\n",
    "# New Feature\n",
    "################################\n",
    "#create new feature based on\n",
    "#percentage of total emails that\n",
    "#are sent to poi's\n",
    "\n",
    "\n",
    "#reset NaN's to 0\n",
    "for i in data_dict:\n",
    "    if data_dict[i]['from_messages'] == 'NaN':\n",
    "        data_dict[i]['from_messages'] = 0\n",
    "    if data_dict[i]['from_this_person_to_poi'] == 'NaN':\n",
    "        data_dict[i]['from_this_person_to_poi'] = 0\n",
    "#if total messages no available.. set new feature to 0 as well\n",
    "    if data_dict[i]['from_messages'] == 0:\n",
    "        data_dict[i]['to_poi_percentage'] = 0\n",
    "    else: #the math!\n",
    "        data_dict[i]['to_poi_percentage'] = float(data_dict[i]['from_this_person_to_poi']) / float(data_dict[i]['from_messages'])\n",
    "        data_dict[i]['to_poi_percentage'] = float(\"{0:.2f}\".format(data_dict[i]['to_poi_percentage']))\n",
    "\n",
    " #print out to check\n",
    "#for i in data_dict:\n",
    "#    pprint(data_dict[i])\n",
    "\n",
    "################################\n",
    "### Extract features and labels from dataset for local testing\n",
    "################################\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "######################\n",
    "#selectpercentile\n",
    "#####################\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "selector = SelectPercentile(f_classif, percentile=30)\n",
    "selector.fit(features, labels)\n",
    "features = selector.transform(features)\n",
    "print selector._get_support_mask()\n",
    "print selector.scores_\n",
    "\n",
    "## f_classif scores -> (salary, 18.3)(bonus, 20.8)(excersied stock options, 24.8)\n",
    "# (to poi percentage, 16.6)(long term incentive, 9.9)(expenses,6.1)\n",
    "# (director fees, 2.12)(total payments, 8.8)\n",
    "################################\n",
    "# scaling\n",
    "################################\n",
    "\n",
    "#minmaxscalerr\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "#features = scaler.fit_transform(features)\n",
    "#print scaler.min_\n",
    "#print scaler.scale_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################\n",
    "# outlier removal\n",
    "################################\n",
    "from sklearn import linear_model\n",
    "\n",
    "#outlier class (from lessons)\n",
    "# def outlierCleaner(predictions, ages, net_worths):\n",
    "#     cleaned_data = []\n",
    "#     errors = (net_worths - predictions)**2\n",
    "#     cleaned_data = zip(ages, net_worths, errors)\n",
    "#     cleaned_data = sorted(cleaned_data, key = lambda x: x[2], reverse = True)\n",
    "#     limit = int(len(net_worths)*0.1)\n",
    "#     return cleaned_data[limit:]\n",
    "#\n",
    "# from sklearn import svm\n",
    "# #reg = GaussianNB()\n",
    "# #reg.fit(features, labels)\n",
    "# reg = svm.LinearSVC()\n",
    "# reg.fit(features, labels)\n",
    "#\n",
    "#\n",
    "# cleaned_data = []\n",
    "# try:\n",
    "#     predictions = reg.predict(features)\n",
    "#     cleaned_data = outlierCleaner(predictions, features, labels)\n",
    "# except NameError:\n",
    "#     print \"your regression object doesn't exist, or isn't name reg\"\n",
    "#     print \"can't make predictions to use in identifying outliers\"\n",
    "\n",
    "\n",
    "#try:\n",
    "#    plt.plot(features, reg.predict(features), color=\"blue\")\n",
    "#except NameError:\n",
    "#    pass\n",
    "#plt.scatter(features, labels)\n",
    "#plt.show()\n",
    "\n",
    "#print len(features)\n",
    "# features_cleaned = numpy.array([e[0] for e in cleaned_data])\n",
    "# labels_cleaned = numpy.array([e[1] for e in cleaned_data])\n",
    "\n",
    "#try:\n",
    "#    plt.plot(features_cleaned, reg.predict(features_cleaned), color=\"blue\")\n",
    "#except NameError:\n",
    "#    pass\n",
    "#plt.scatter(features_cleaned, labels_cleaned)\n",
    "#plt.show()\n",
    "\n",
    "#set features/labels to cleaned set\n",
    "# features = features_cleaned\n",
    "# labels = labels_cleaned\n",
    "\n",
    "\n",
    "#print len(features)\n",
    "#split train/test\n",
    "features_train,features_test, labels_train,labels_test = train_test_split(features,labels,\n",
    "                                            test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "################################\n",
    "# classifying\n",
    "################################\n",
    "\n",
    "#GaussianNB   A- 23 P-14 R-95\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "#clf = GaussianNB()\n",
    "#clf.fit(features_train, labels_train)\n",
    "\n",
    "#SVM   A-  R-  P-\n",
    "#from sklearn import svm, grid_search\n",
    "#parameters = {'C':[1, 5, 10]}\n",
    "#svr = svm.LinearSVC()\n",
    "#clf = grid_search.GridSearchCV(svr, parameters)\n",
    "#clf = svm.LinearSVC(multi_class='crammer_singer')\n",
    "#clf.fit(features_train, labels_train)\n",
    "\n",
    "#RandomForest   A- 87 P- 55 R- 17\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn import grid_search\n",
    "#clf = RandomForestClassifier(n_estimators=30, min_samples_split=5)\n",
    "#parameters = {'min_samples_split':[2,3,4,5,6], 'n_estimators': [10,20]}\n",
    "#random = RandomForestClassifier()\n",
    "#clf = grid_search.GridSearchCV(random, parameters)\n",
    "#clf.fit(features_train, labels_train)\n",
    "\n",
    "#adaboost  A- 85 P- 39 R- 32\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import grid_search\n",
    "#ada = AdaBoostClassifier()\n",
    "#parameters = {'n_estimators':[10,50,100], 'random_state': [None, 0, 42, 138]}\n",
    "#clf = grid_search.GridSearchCV(ada, parameters)\n",
    "clf = AdaBoostClassifier(n_estimators=50, random_state=138)\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "\n",
    "#K-Means_clustering R-.28. P-.21 f1-.24\n",
    "#from sklearn.cluster import KMeans\n",
    "#clf = KMeans(n_clusters = 2)\n",
    "#clf.fit(features_train, labels_test)\n",
    "#print clf.predict(features_test)\n",
    "#print labels_test\n",
    "\n",
    "#DecisionTree  #R- .32 P- .33 f1 - .33\n",
    "from sklearn import grid_search, tree\n",
    "#parameters = {'min_samples_split':[2,3,4,5,6,7,8,9], 'min_samples_leaf':[1,2,3], 'random_state':[None, 0, 42] }\n",
    "#tree = tree.DecisionTreeClassifier()\n",
    "#clf = grid_search.GridSearchCV(tree, parameters)\n",
    "#clf = tree.DecisionTreeClassifier(min_samples_split = 5, random_state=42)\n",
    "#clf = clf.fit(features_train, labels_train)\n",
    "#clf.predict(features_test)\n",
    "#print clf.best_estimator_\n",
    "#print labels_test\n",
    "#print clf.predict(features_test)\n",
    "\n",
    "\n",
    "#try:\n",
    "#    plt.plot(features, clf.predict(features), color=\"blue\")\n",
    "#except NameError:\n",
    "#    pass\n",
    "#plt.scatter(features, labels)\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################\n",
    "# validation\n",
    "################################\n",
    "predict = clf.predict(features_test)\n",
    "print 'accuracy: %f' %accuracy_score(labels_test, predict)\n",
    "print 'Precision: %f' %precision_score(labels_test, predict)\n",
    "print 'Recall: %f' %recall_score(labels_test, predict)\n",
    "print 'F1 score: %f' %f1_score(labels_test, predict)\n",
    "\n",
    "true_pos = 0\n",
    "true_neg = 0\n",
    "false_pos = 0\n",
    "false_neg = 0\n",
    "#find counts\n",
    "for num in range(0,len(predict)):\n",
    "    if predict[num] == 1 and labels_test[num]== 1:\n",
    "        true_pos += 1\n",
    "    if predict[num] == 0 and labels_test[num]== 0:\n",
    "        true_neg += 1\n",
    "    if predict[num] == 1 and labels_test[num]== 0:\n",
    "        false_pos += 1\n",
    "    if predict[num] == 0 and labels_test[num]== 1:\n",
    "        false_neg += 1\n",
    "#prints\n",
    "print 'True Positives: %d' %true_pos\n",
    "print 'True Negatives: %d' %true_neg\n",
    "print 'False Positive: %d' %false_pos\n",
    "print 'False Negative: %d' %false_neg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################\n",
    "# tune classifier\n",
    "################################\n",
    "test_classifier(clf, my_dataset, features_list)\n",
    "\n",
    "\n",
    "\n",
    "################################\n",
    "# dump data\n",
    "################################\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
