{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing and evaluating numerous machine learning techniques to determine best option for predicting fruad occurances in Enron email dataset.  The most efficient predictor ended up being an Adaboost algorithm with 50 n_estimators.  This method using decision tree as a 'weak learner' came out with about 85% accuracy, p-value of 39, and an r-squared of around 32.  Originally conducted for Udacity Nanodegree project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146\n",
      "[ True  True  True False False False False False]\n",
      "[ 18.28968404  20.79225205  24.81507973  16.57961842   9.92218601\n",
      "   6.09417331   2.1263278    8.76567569]\n",
      "accuracy: 0.860465\n",
      "Precision: 0.500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.333333\n",
      "F1 score: 0.400000\n",
      "True Positives: 2\n",
      "True Negatives: 35\n",
      "False Positive: 2\n",
      "False Negative: 4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import numpy\n",
    "sys.path.append(\"../data/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import test_classifier, dump_classifier_and_data\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import *\n",
    "\n",
    "################################\n",
    "# feature selection\n",
    "################################\n",
    "features_list = ['poi','salary', 'bonus', 'exercised_stock_options', 'to_poi_percentage',\n",
    "                 'long_term_incentive', 'expenses', 'director_fees', 'total_payments']\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "data_dict = pickle.load(open(\"../data/final_project_dataset.pkl\", \"r\") )\n",
    "#n of data points\n",
    "print len(data_dict)\n",
    "\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "data_dict.pop('TOTAL',0)  #first outlier..  invalid spreadsheet input\n",
    "my_dataset = data_dict\n",
    "\n",
    "################################\n",
    "# New Feature\n",
    "################################\n",
    "#create new feature based on\n",
    "#percentage of total emails that\n",
    "#are sent to poi's\n",
    "\n",
    "\n",
    "#reset NaN's to 0\n",
    "for i in data_dict:\n",
    "    if data_dict[i]['from_messages'] == 'NaN':\n",
    "        data_dict[i]['from_messages'] = 0\n",
    "    if data_dict[i]['from_this_person_to_poi'] == 'NaN':\n",
    "        data_dict[i]['from_this_person_to_poi'] = 0\n",
    "#if total messages no available.. set new feature to 0 as well\n",
    "    if data_dict[i]['from_messages'] == 0:\n",
    "        data_dict[i]['to_poi_percentage'] = 0\n",
    "    else: #the math!\n",
    "        data_dict[i]['to_poi_percentage'] = float(data_dict[i]['from_this_person_to_poi']) / float(data_dict[i]['from_messages'])\n",
    "        data_dict[i]['to_poi_percentage'] = float(\"{0:.2f}\".format(data_dict[i]['to_poi_percentage']))\n",
    "\n",
    " #print out to check\n",
    "#for i in data_dict:\n",
    "#    pprint(data_dict[i])\n",
    "\n",
    "################################\n",
    "### Extract features and labels from dataset for local testing\n",
    "################################\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "######################\n",
    "#selectpercentile\n",
    "#####################\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "selector = SelectPercentile(f_classif, percentile=30)\n",
    "selector.fit(features, labels)\n",
    "features = selector.transform(features)\n",
    "print selector._get_support_mask()\n",
    "print selector.scores_\n",
    "\n",
    "## f_classif scores -> (salary, 18.3)(bonus, 20.8)(excersied stock options, 24.8)\n",
    "# (to poi percentage, 16.6)(long term incentive, 9.9)(expenses,6.1)\n",
    "# (director fees, 2.12)(total payments, 8.8)\n",
    "################################\n",
    "# scaling\n",
    "################################\n",
    "\n",
    "#minmaxscalerr\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "#features = scaler.fit_transform(features)\n",
    "#print scaler.min_\n",
    "#print scaler.scale_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################\n",
    "# outlier removal\n",
    "################################\n",
    "from sklearn import linear_model\n",
    "\n",
    "#outlier class (from lessons)\n",
    "# def outlierCleaner(predictions, ages, net_worths):\n",
    "#     cleaned_data = []\n",
    "#     errors = (net_worths - predictions)**2\n",
    "#     cleaned_data = zip(ages, net_worths, errors)\n",
    "#     cleaned_data = sorted(cleaned_data, key = lambda x: x[2], reverse = True)\n",
    "#     limit = int(len(net_worths)*0.1)\n",
    "#     return cleaned_data[limit:]\n",
    "#\n",
    "# from sklearn import svm\n",
    "# #reg = GaussianNB()\n",
    "# #reg.fit(features, labels)\n",
    "# reg = svm.LinearSVC()\n",
    "# reg.fit(features, labels)\n",
    "#\n",
    "#\n",
    "# cleaned_data = []\n",
    "# try:\n",
    "#     predictions = reg.predict(features)\n",
    "#     cleaned_data = outlierCleaner(predictions, features, labels)\n",
    "# except NameError:\n",
    "#     print \"your regression object doesn't exist, or isn't name reg\"\n",
    "#     print \"can't make predictions to use in identifying outliers\"\n",
    "\n",
    "\n",
    "#try:\n",
    "#    plt.plot(features, reg.predict(features), color=\"blue\")\n",
    "#except NameError:\n",
    "#    pass\n",
    "#plt.scatter(features, labels)\n",
    "#plt.show()\n",
    "\n",
    "#print len(features)\n",
    "# features_cleaned = numpy.array([e[0] for e in cleaned_data])\n",
    "# labels_cleaned = numpy.array([e[1] for e in cleaned_data])\n",
    "\n",
    "#try:\n",
    "#    plt.plot(features_cleaned, reg.predict(features_cleaned), color=\"blue\")\n",
    "#except NameError:\n",
    "#    pass\n",
    "#plt.scatter(features_cleaned, labels_cleaned)\n",
    "#plt.show()\n",
    "\n",
    "#set features/labels to cleaned set\n",
    "# features = features_cleaned\n",
    "# labels = labels_cleaned\n",
    "\n",
    "\n",
    "#print len(features)\n",
    "#split train/test\n",
    "features_train,features_test, labels_train,labels_test = train_test_split(features,labels,\n",
    "                                            test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "################################\n",
    "# classifying\n",
    "################################\n",
    "\n",
    "#GaussianNB   A- 23 P-14 R-95\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "#clf = GaussianNB()\n",
    "#clf.fit(features_train, labels_train)\n",
    "\n",
    "#SVM   A-  R-  P-\n",
    "#from sklearn import svm, grid_search\n",
    "#parameters = {'C':[1, 5, 10]}\n",
    "#svr = svm.LinearSVC()\n",
    "#clf = grid_search.GridSearchCV(svr, parameters)\n",
    "#clf = svm.LinearSVC(multi_class='crammer_singer')\n",
    "#clf.fit(features_train, labels_train)\n",
    "\n",
    "#RandomForest   A- 87 P- 55 R- 17\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn import grid_search\n",
    "#clf = RandomForestClassifier(n_estimators=30, min_samples_split=5)\n",
    "#parameters = {'min_samples_split':[2,3,4,5,6], 'n_estimators': [10,20]}\n",
    "#random = RandomForestClassifier()\n",
    "#clf = grid_search.GridSearchCV(random, parameters)\n",
    "#clf.fit(features_train, labels_train)\n",
    "\n",
    "#adaboost  A- 85 P- 39 R- 32\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import grid_search\n",
    "#ada = AdaBoostClassifier()\n",
    "#parameters = {'n_estimators':[10,50,100], 'random_state': [None, 0, 42, 138]}\n",
    "#clf = grid_search.GridSearchCV(ada, parameters)\n",
    "clf = AdaBoostClassifier(n_estimators=50, random_state=138)\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "\n",
    "#K-Means_clustering R-.28. P-.21 f1-.24\n",
    "#from sklearn.cluster import KMeans\n",
    "#clf = KMeans(n_clusters = 2)\n",
    "#clf.fit(features_train, labels_test)\n",
    "#print clf.predict(features_test)\n",
    "#print labels_test\n",
    "\n",
    "#DecisionTree  #R- .32 P- .33 f1 - .33\n",
    "from sklearn import grid_search, tree\n",
    "#parameters = {'min_samples_split':[2,3,4,5,6,7,8,9], 'min_samples_leaf':[1,2,3], 'random_state':[None, 0, 42] }\n",
    "#tree = tree.DecisionTreeClassifier()\n",
    "#clf = grid_search.GridSearchCV(tree, parameters)\n",
    "#clf = tree.DecisionTreeClassifier(min_samples_split = 5, random_state=42)\n",
    "#clf = clf.fit(features_train, labels_train)\n",
    "#clf.predict(features_test)\n",
    "#print clf.best_estimator_\n",
    "#print labels_test\n",
    "#print clf.predict(features_test)\n",
    "\n",
    "\n",
    "#try:\n",
    "#    plt.plot(features, clf.predict(features), color=\"blue\")\n",
    "#except NameError:\n",
    "#    pass\n",
    "#plt.scatter(features, labels)\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################\n",
    "# validation\n",
    "################################\n",
    "predict = clf.predict(features_test)\n",
    "print 'accuracy: %f' %accuracy_score(labels_test, predict)\n",
    "print 'Precision: %f' %precision_score(labels_test, predict)\n",
    "print 'Recall: %f' %recall_score(labels_test, predict)\n",
    "print 'F1 score: %f' %f1_score(labels_test, predict)\n",
    "\n",
    "true_pos = 0\n",
    "true_neg = 0\n",
    "false_pos = 0\n",
    "false_neg = 0\n",
    "#find counts\n",
    "for num in range(0,len(predict)):\n",
    "    if predict[num] == 1 and labels_test[num]== 1:\n",
    "        true_pos += 1\n",
    "    if predict[num] == 0 and labels_test[num]== 0:\n",
    "        true_neg += 1\n",
    "    if predict[num] == 1 and labels_test[num]== 0:\n",
    "        false_pos += 1\n",
    "    if predict[num] == 0 and labels_test[num]== 1:\n",
    "        false_neg += 1\n",
    "#prints\n",
    "print 'True Positives: %d' %true_pos\n",
    "print 'True Negatives: %d' %true_neg\n",
    "print 'False Positive: %d' %false_pos\n",
    "print 'False Negative: %d' %false_neg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################\n",
    "# tune classifier\n",
    "################################\n",
    "test_classifier(clf, my_dataset, features_list)\n",
    "\n",
    "\n",
    "\n",
    "################################\n",
    "# dump data\n",
    "################################\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
