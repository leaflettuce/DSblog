{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part three of my attempt to predict NCAA tourney results based on past game data. As cleaning and var creation is out of the way now, this segment will focus on fitting the data to different classifiers in scikit-lelarn and tweaking parameters to determine a classification model with the best prediction power... without overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import math\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import grid_search\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import list is large here as I threw in a large amount of the classifiers included in sklearn. As noticeable later, once I get the trainn and test data split I toss it into several different models and use the accuracy scores of each to fine tune//pick a final model.  Data importing next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AST</th>\n",
       "      <th>AST_Diff</th>\n",
       "      <th>BLK</th>\n",
       "      <th>BLK_Diff</th>\n",
       "      <th>Coach</th>\n",
       "      <th>DR</th>\n",
       "      <th>DR_Diff</th>\n",
       "      <th>FGP</th>\n",
       "      <th>FGP3</th>\n",
       "      <th>FGP3_Diff</th>\n",
       "      <th>...</th>\n",
       "      <th>PPG_Diff</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Rank_Diff</th>\n",
       "      <th>Result</th>\n",
       "      <th>SEED_Diff</th>\n",
       "      <th>STL</th>\n",
       "      <th>STL_Diff</th>\n",
       "      <th>Season</th>\n",
       "      <th>Seed</th>\n",
       "      <th>TeamID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>4.176471</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>mark_gottfried</td>\n",
       "      <td>26.411765</td>\n",
       "      <td>3.911765</td>\n",
       "      <td>0.444393</td>\n",
       "      <td>0.347418</td>\n",
       "      <td>0.040750</td>\n",
       "      <td>...</td>\n",
       "      <td>18.539216</td>\n",
       "      <td>3.0</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.235294</td>\n",
       "      <td>1.401961</td>\n",
       "      <td>2003</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.380952</td>\n",
       "      <td>6.047619</td>\n",
       "      <td>4.095238</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>rick_stansbury</td>\n",
       "      <td>26.380952</td>\n",
       "      <td>3.880952</td>\n",
       "      <td>0.495357</td>\n",
       "      <td>0.361980</td>\n",
       "      <td>0.055311</td>\n",
       "      <td>...</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.285714</td>\n",
       "      <td>3.452381</td>\n",
       "      <td>2003</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.400000</td>\n",
       "      <td>4.066667</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>eddie_sutton</td>\n",
       "      <td>24.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.474798</td>\n",
       "      <td>0.382793</td>\n",
       "      <td>0.076124</td>\n",
       "      <td>...</td>\n",
       "      <td>17.233333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.650000</td>\n",
       "      <td>3.816667</td>\n",
       "      <td>2003</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.590909</td>\n",
       "      <td>5.257576</td>\n",
       "      <td>3.818182</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>rick_barnes</td>\n",
       "      <td>26.636364</td>\n",
       "      <td>4.136364</td>\n",
       "      <td>0.456127</td>\n",
       "      <td>0.343126</td>\n",
       "      <td>0.036458</td>\n",
       "      <td>...</td>\n",
       "      <td>22.060606</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.954545</td>\n",
       "      <td>1.121212</td>\n",
       "      <td>2003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.590909</td>\n",
       "      <td>5.257576</td>\n",
       "      <td>3.818182</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>rick_barnes</td>\n",
       "      <td>26.636364</td>\n",
       "      <td>4.136364</td>\n",
       "      <td>0.456127</td>\n",
       "      <td>0.343126</td>\n",
       "      <td>0.036458</td>\n",
       "      <td>...</td>\n",
       "      <td>22.060606</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.954545</td>\n",
       "      <td>1.121212</td>\n",
       "      <td>2003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         AST  AST_Diff       BLK  BLK_Diff           Coach         DR  \\\n",
       "0  14.000000  4.666667  4.176471  0.676471  mark_gottfried  26.411765   \n",
       "1  15.380952  6.047619  4.095238  0.595238  rick_stansbury  26.380952   \n",
       "2  13.400000  4.066667  5.750000  2.250000    eddie_sutton  24.500000   \n",
       "3  14.590909  5.257576  3.818182  0.318182     rick_barnes  26.636364   \n",
       "4  14.590909  5.257576  3.818182  0.318182     rick_barnes  26.636364   \n",
       "\n",
       "    DR_Diff       FGP      FGP3  FGP3_Diff   ...     PPG_Diff  Rank  \\\n",
       "0  3.911765  0.444393  0.347418   0.040750   ...    18.539216   3.0   \n",
       "1  3.880952  0.495357  0.361980   0.055311   ...    17.500000   3.0   \n",
       "2  2.000000  0.474798  0.382793   0.076124   ...    17.233333   3.0   \n",
       "3  4.136364  0.456127  0.343126   0.036458   ...    22.060606   3.0   \n",
       "4  4.136364  0.456127  0.343126   0.036458   ...    22.060606   3.0   \n",
       "\n",
       "   Rank_Diff Result  SEED_Diff       STL  STL_Diff  Season  Seed  TeamID  \n",
       "0         35      1        9.0  7.235294  1.401961    2003  10.0    1104  \n",
       "1         21      1        4.0  9.285714  3.452381    2003   5.0    1280  \n",
       "2         19      1        5.0  9.650000  3.816667    2003   6.0    1329  \n",
       "3          1      1        0.0  6.954545  1.121212    2003   1.0    1400  \n",
       "4          1      1        0.0  6.954545  1.121212    2003   1.0    1400  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/train_data_diff.csv')\n",
    "data_dir = '../data/'\n",
    "sample_sub = pd.read_csv(data_dir + 'SampleSubmissionStage1.csv')\n",
    "n_test_games = len(sample_sub)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is a handful of helper function that will facilitate in extracting the year and teams from Kaggles sample submission. This will allow me to use their sample submission to set predictions and submit to the competition. The get_year_t1_t2 function came from the Basic Starter Kernel of the competition site by Julie Elliott.  \n",
    "\n",
    "The other two functions will are used to create a win/loss ration which I missed in earlier manipulation. Get_stat will be used to iterate over the sample sub teams and retrieve the appropriate stat differentials as this is what is used in the training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_count(teamID, year, wl):\n",
    "    if wl == 1:\n",
    "        try:\n",
    "            return df[(df.TeamID == teamID) & (df.Season == year) & (df.Result == 1)].TeamID.value_counts().iloc[0]\n",
    "        except IndexError:\n",
    "            return 0\n",
    "    else:\n",
    "        try:\n",
    "            return df[(df.TeamID == teamID) & (df.Season == year) & (df.Result == 0)].TeamID.value_counts().iloc[0]\n",
    "        except IndexError:\n",
    "            return 0\n",
    "\n",
    "'''FUNCTIONS FOR TEST DATA'''\n",
    "def get_year_t1_t2(ID):\n",
    "    \"\"\"Return a tuple with ints `year`, `team1` and `team2`.\"\"\"\n",
    "    return (int(x) for x in ID.split('_'))\n",
    "\n",
    "\n",
    "def get_stat(stat, t1, t2, year):\n",
    "    if not math.isnan(df[(df.TeamID == t1) & (df.Season == year)][stat].mean() - df[(df.TeamID == t2) & (df.Season == year)][stat].mean()):\n",
    "        return (df[(df.TeamID == t1) & (df.Season == year)][stat].mean() - df[(df.TeamID == t2) & (df.Season == year)][stat].mean())  \n",
    "    else:\n",
    "        return df[(df.TeamID == t1)][stat].mean() - df[(df.TeamID == t2)][stat].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little more housecleaning before actual model experimentation.  This will set the win loss ratio for each team in the training dataframe and turn each location variable into categorical dummies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AST</th>\n",
       "      <th>AST_Diff</th>\n",
       "      <th>BLK</th>\n",
       "      <th>BLK_Diff</th>\n",
       "      <th>Coach</th>\n",
       "      <th>DR</th>\n",
       "      <th>DR_Diff</th>\n",
       "      <th>FGP</th>\n",
       "      <th>FGP3</th>\n",
       "      <th>FGP3_Diff</th>\n",
       "      <th>...</th>\n",
       "      <th>SEED_Diff</th>\n",
       "      <th>STL</th>\n",
       "      <th>STL_Diff</th>\n",
       "      <th>Season</th>\n",
       "      <th>Seed</th>\n",
       "      <th>TeamID</th>\n",
       "      <th>WLRatio</th>\n",
       "      <th>A</th>\n",
       "      <th>H</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>4.176471</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>mark_gottfried</td>\n",
       "      <td>26.411765</td>\n",
       "      <td>3.911765</td>\n",
       "      <td>0.444393</td>\n",
       "      <td>0.347418</td>\n",
       "      <td>0.040750</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.235294</td>\n",
       "      <td>1.401961</td>\n",
       "      <td>2003</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1104</td>\n",
       "      <td>-0.258929</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.380952</td>\n",
       "      <td>6.047619</td>\n",
       "      <td>4.095238</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>rick_stansbury</td>\n",
       "      <td>26.380952</td>\n",
       "      <td>3.880952</td>\n",
       "      <td>0.495357</td>\n",
       "      <td>0.361980</td>\n",
       "      <td>0.055311</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.285714</td>\n",
       "      <td>3.452381</td>\n",
       "      <td>2003</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1280</td>\n",
       "      <td>-0.133929</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.400000</td>\n",
       "      <td>4.066667</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>eddie_sutton</td>\n",
       "      <td>24.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.474798</td>\n",
       "      <td>0.382793</td>\n",
       "      <td>0.076124</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.650000</td>\n",
       "      <td>3.816667</td>\n",
       "      <td>2003</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1329</td>\n",
       "      <td>-0.214286</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.590909</td>\n",
       "      <td>5.257576</td>\n",
       "      <td>3.818182</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>rick_barnes</td>\n",
       "      <td>26.636364</td>\n",
       "      <td>4.136364</td>\n",
       "      <td>0.456127</td>\n",
       "      <td>0.343126</td>\n",
       "      <td>0.036458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.954545</td>\n",
       "      <td>1.121212</td>\n",
       "      <td>2003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1400</td>\n",
       "      <td>-0.171429</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.590909</td>\n",
       "      <td>5.257576</td>\n",
       "      <td>3.818182</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>rick_barnes</td>\n",
       "      <td>26.636364</td>\n",
       "      <td>4.136364</td>\n",
       "      <td>0.456127</td>\n",
       "      <td>0.343126</td>\n",
       "      <td>0.036458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.954545</td>\n",
       "      <td>1.121212</td>\n",
       "      <td>2003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1400</td>\n",
       "      <td>-0.171429</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         AST  AST_Diff       BLK  BLK_Diff           Coach         DR  \\\n",
       "0  14.000000  4.666667  4.176471  0.676471  mark_gottfried  26.411765   \n",
       "1  15.380952  6.047619  4.095238  0.595238  rick_stansbury  26.380952   \n",
       "2  13.400000  4.066667  5.750000  2.250000    eddie_sutton  24.500000   \n",
       "3  14.590909  5.257576  3.818182  0.318182     rick_barnes  26.636364   \n",
       "4  14.590909  5.257576  3.818182  0.318182     rick_barnes  26.636364   \n",
       "\n",
       "    DR_Diff       FGP      FGP3  FGP3_Diff ...  SEED_Diff       STL  STL_Diff  \\\n",
       "0  3.911765  0.444393  0.347418   0.040750 ...        9.0  7.235294  1.401961   \n",
       "1  3.880952  0.495357  0.361980   0.055311 ...        4.0  9.285714  3.452381   \n",
       "2  2.000000  0.474798  0.382793   0.076124 ...        5.0  9.650000  3.816667   \n",
       "3  4.136364  0.456127  0.343126   0.036458 ...        0.0  6.954545  1.121212   \n",
       "4  4.136364  0.456127  0.343126   0.036458 ...        0.0  6.954545  1.121212   \n",
       "\n",
       "  Season  Seed  TeamID   WLRatio  A  H  N  \n",
       "0   2003  10.0    1104 -0.258929  0  0  1  \n",
       "1   2003   5.0    1280 -0.133929  0  0  1  \n",
       "2   2003   6.0    1329 -0.214286  0  1  0  \n",
       "3   2003   1.0    1400 -0.171429  0  1  0  \n",
       "4   2003   1.0    1400 -0.171429  1  0  0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''WIN LOSS RATIO'''\n",
    "df['WLRatio'] = df.apply(lambda row: get_count(row.TeamID, row.Season, 1)/ (get_count(row.TeamID, row.Season, 0) + get_count(row.TeamID, row.Season, 1)).astype('float') - \\\n",
    "      get_count(row.OtherTeamID, row.Season, 1)/ (get_count(row.OtherTeamID, row.Season, 0) + get_count(row.OtherTeamID, row.Season, 1)).astype('float'), axis = 1)\n",
    "\n",
    "'''SET DUMMIES'''\n",
    "loc_dummies = pd.get_dummies(df.Loc)\n",
    "df = pd.concat([df, loc_dummies], axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last to the interesting part.. Setting the training data and splitting into a test set.  This will allow for a good amount (66%) of the data to be used in training the model, with the remainder available to test the accuracy of the fit.  \n",
    "\n",
    "I had tried a feature selector with a few of the final models, yet inclusion of all 11 features yeilded better results over the versions with only the top 20% influencers... So this is commented out. While it might be useful in explaination of the predictions (PPG and FGP are by far the top two influenctial variables), I opted to include even minimally influental variables in that resulted in higher accuracy of the model.  \n",
    "\n",
    "The only unexpected change is the drop of offensive rebounds from the variable list.. They were way over the place and had not correlation with winning the game.  Similarly, game location can not be deciphered in the kaggle submission file, so I drop this dummy var as well.  I hope to include it back in for the final test when the tourny teams are selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15740    0\n",
       "7233     1\n",
       "14898    0\n",
       "12298    0\n",
       "1371     1\n",
       "Name: Result, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''FEATURE SELECTION'''\n",
    "X_train = df[['PPG_Diff', 'FGP_Diff', 'AST_Diff', 'FGP3_Diff', 'SEED_Diff',\n",
    "             'FTP_Diff', 'DR_Diff', 'STL_Diff', 'BLK_Diff', 'Rank_Diff', 'WLRatio']]\n",
    "y_train = df['Result']\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "'''SELECTION SCORES'''\n",
    "#X_new = SelectPercentile(percentile = 20).fit_transform(X_train, y_train)\n",
    "\n",
    "'''TRAIN-TEST SPLIT - (for testing locally)'''\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n",
    "X_train.head()\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally to the model fitting.  Most are commented out which indicates that they were not asd powerful and accurate as the final selected: SVC with a linear kernel. I worry simply judging by accuracy is not the most efficient and apt method to deciding between fits, but it appears to have a direct relation to how highly the final submission scores on Kaggle as well. \n",
    "\n",
    "While Random Forest and Adaboost both have great acucuracy.. as excpected with a dataset like this.. I was excited to see SVC b so efficient at prediction based on the inputs.  The clf.score reported .9993. An insanely accurate score which actually ends up worrying me.  I fear this would mean it is largely overfit, but as the test data was shuffled and so large, I find that it might be acceptable?   Regardless, this is the final model I went with! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99894273127753308"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''GAUSSIAN NAIVE BAYES'''\n",
    "#clf = GaussianNB()\n",
    "#clf.fit(X_train, y_train)\n",
    "#clf.score(X_test, y_test)\n",
    "\n",
    "\n",
    "'''LOGISTIC REGRESSION'''\n",
    "#logreg = LogisticRegression()\n",
    "#params = {'C': np.logspace(start=-5, stop=3, num=9)}\n",
    "#clf = GridSearchCV(logreg, params, scoring='neg_log_loss', refit=True)\n",
    "#clf.fit(X_train, y_train)\n",
    "#print('Best log_loss: {:.4}, with best C: {}'\n",
    "#      .format(clf.best_score_, clf.best_params_['C']))\n",
    "\n",
    "\n",
    "'''LOG REG CV'''\n",
    "#clf = LogisticRegressionCV()\n",
    "#clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "'''TRAIN MODEL - RANDOM FOREST'''\n",
    "#clf = RandomForestClassifier(random_state=42)\n",
    "#clf.fit(X_train, y_train)\n",
    "#clf.feature_importances_\n",
    "#clf.score(X_test, y_test)\n",
    "\n",
    "\n",
    "'''ADABOOST'''\n",
    "#ada = AdaBoostClassifier()\n",
    "#parameters = {'n_estimators':[10,50,100], 'random_state': [None, 0, 42, 138]}\n",
    "#clf = grid_search.GridSearchCV(ada, parameters)\n",
    "#clf = AdaBoostClassifier(n_estimators=50, random_state=138)\n",
    "#clf.fit(X_train, y_train)\n",
    "\n",
    "#clf.score(X_test, y_test)\n",
    "\n",
    "\n",
    "'''K Nearest Neighbor'''\n",
    "#clf = KNeighborsClassifier(n_neighbors=5)\n",
    "#clf.fit(X_train, y_train) \n",
    "\n",
    "#clf.score(X_test, y_test)\n",
    "\n",
    "\n",
    "'''Support Vector Machine''' \n",
    "clf = svm.SVC(kernel='linear', gamma=0.7, C=1.0, probability = True)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "clf.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next segment is partially thanks to Julie Elliott again with her starter kernel.  I was at a loss of how to easily and quickly iterate through the sample submission and report the model predicted probabilities. I tweaked it to utilize the earlier created stat differential function to create appropriate variables to run the model on.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''SET TEST DATAFRAME'''\n",
    "X_sub = np.zeros(shape=(n_test_games, 11))\n",
    "\n",
    "\n",
    "'''SETTING FEATURES'''\n",
    "stat_list = ['PPG', 'FGP', 'AST', 'FGP3', 'Seed', 'FTP', 'DR', 'STL', 'BLK', 'Rank'] \n",
    "\n",
    "for ii, row in sample_sub.iterrows():\n",
    "    year, t1, t2 = get_year_t1_t2(row.ID)\n",
    "    col_num = 0\n",
    "    \n",
    "    for team_stat in stat_list:\n",
    "        X_sub[ii, col_num] = get_stat(team_stat, t1, t2, year)\n",
    "        col_num += 1\n",
    "        \n",
    "    X_sub[ii, col_num] =  get_count(t1, year, 1)/ (get_count(t1, year, 0) + get_count(t1, year, 1)).astype('float') - \\\n",
    "     get_count(t2, year, 1)/ (get_count(t2, year, 0) + get_count(t2, year, 1)).astype('float')\n",
    "\n",
    "      \n",
    "'''EDIT NaN's and infinite values'''\n",
    "imp = Imputer(missing_values='NaN', strategy='median', axis=1) \n",
    "imp.fit(X_sub)\n",
    "X_sub = imp.fit_transform(X_sub)\n",
    "X_sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to the final part of the project.. Using the model on the Kaggle sample submission and turning in on the competition site to see how I fare.  Predict_proba is used here as the competition is graded in a log loss method which scores based on the probability scores set to each game. I then clip the extreme ends of the scores as log loss methods highly deduct for highly certain false positives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''MAKE PREDICTIONS'''\n",
    "preds = clf.predict_proba(X_sub)[:,1]\n",
    "\n",
    "'''CLIP PREDICTIONS'''\n",
    "clipped_preds = np.clip(preds, 0.05, 0.95)\n",
    "sample_sub.Pred = clipped_preds\n",
    "\n",
    "'''WRITE TO CSV'''\n",
    "#sample_sub.to_csv('SVC_11FD_clipped_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Kaggle Results\n",
    "So the extremely high accuracy of the model score on here does not directly relate to the same in a log loss environment.  I scored 0.633 on the competition site.. I believe this equates to around 92% accuracy? Slightly worse than the Basic starter kernel which only used Seed differential in predicting probabilities.  \n",
    "\n",
    "This is not to say it will not be effective in round two of the competition where current tourney data is unavaible.  I believe my model's use of only regular season data is both a pro and a con.  It can allow me to appropriately fit each game in the 2018 bracket based soley on regular season data from this year. Conversely, it also does not take into account the randomness that is seen within Tourny games- Hence the 'madness' of the bracket. \n",
    "\n",
    "If time becomes available again before the begining of the tourney, I plan to find a way to include betting market odds of the games into the training data. I also think utilizing those dummy location variables and possibly including some lagged past season tourney placements.\n",
    "\n",
    "###How Did My Bracket Do: (UPCOMING)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
