{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being a big fan of fantasy novels, I've always had an interest in how characters within books with massive character lists all interweave and connect together. I've also had interest for awhile now in visualizing some type of complex network with networkX in Python. Oathbringer is the most recent book from Brandon Sanderson's Stormlight Archive series and it was a perfect option for me to combine both of these interests. The following is the code I used to parse through the etext of the novel and create a character co-occurence network diagram. Although certain decisions made throughout the process may not perfect for representing direct 'co-occurrences', I found the resulting visual to be an interesting look at relationships seen throughout the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Created by Andy Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get to the code, I thought I'd post the final resulting image here. This is the final version I was able to achieve with only some minor nlp with regex and formulating the text into a matrix to work with the networkX package.\n",
    "\n",
    "<img src = \"../img/stormlight/network.png\" style = \"width: 800px;\"/>",
    " \n",
    "Now, onto the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Networkx works off of matplotlib, so both imports were required above.  Additionally, I end up using a bit of regex to filter some text and use pandas to create the origional co-occurrence matrix that helped formulate the network. \n",
    " \n",
    "So with that, the first step was to import the data in .txt format. This is a direct text file from the ebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../data/oathbringer.txt') as text:\n",
    "    book_text = text.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple enough to read the text file in. I next use the astrisks to split the text document up into a list of text blocks.  I additionally use '-From' to break it apart as well. Each of these indicate either a new chapter or a new Point of view within the book.\n",
    "\n",
    "This was one of the major decision points for this project- How to indicate a 'co-occurrence'.  I opted for counting a co-occurrence as any time someone is in the same POV as another person.. While using some nlp and n-gram length checks could have been a more accurate method to figuring 'co-occurrences', I worried it would be too inaccurate.  As such, I chose this method that errs towards inclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#split at pov changes\n",
    "book_text = book_text.replace('—From', ' * * * ')\n",
    "sections = book_text.split('* * *')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I next remove all dialogue from the text.. this was pretty simple to do with regex and allowed me to not include characters as a co-occurrence on the chance that it was two people talking about a third who was not within the scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove all dialogue\n",
    "cleaned_sections = []\n",
    "\n",
    "for section in sections:\n",
    "    quotes = re.findall(\"“.*?”\", section)\n",
    "    for quote in quotes:\n",
    "        section = section.replace(quote, \" \")\n",
    "    cleaned_sections.append(section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I manually created a list of all the 'important' chatracters in the book. Basically I went through the wiki page and included anyone of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Create characters list and cooccurnce matrix\n",
    "characters = [' Syl ', ' Pattern ', 'Wyndle ', 'Glys ', ' Ivory ', ' Timbre ', 'Stormfather ',\n",
    "              'Nightwatcher ', 'Dalinar ', 'Shallan ', 'Kaladin ', 'Venli ', 'Adolin ', \n",
    "              'Szeth ', 'Navani ', 'Moash ', 'Jasnah ', 'Teft ', 'Renarin ', 'Lift ', \n",
    "              'Taravangian ', 'Wit ', 'Eshonai ', ' Rock ', 'Lopen ', 'Rysn ', 'Sigzil ',\n",
    "              'palona ', 'mem ', 'ellista ', 'kaza ', 'gawx ', 'sheler ', 'rlain ',\n",
    "              'torol ', 'meridas ', 'teleb ', 'gavilar ', 'resi ', 'elit ', 'erraniv ',\n",
    "              'helaran ', 'jakamav ', 'kalishor ', 'salinor ', 'tanalor ', 'tinalar ',\n",
    "              'skar ', 'dabbid ', 'hobber ', 'shen ', 'leyten ', \n",
    "              'drehy ', 'gadol ', 'natam ', 'peet ', 'torfin ', 'yake ', 'baxil ', \n",
    "              'roshone ', 'tavinar ', 'istow ', 'dukar ', 'gavinor ', 'gaz ', 'ghenna ', \n",
    "              'hoid ', 'inadara', 'isasik', 'ishikk', 'jenet', 'kadash', 'kalami', \n",
    "              'khal ', 'khriss', 'laral ', 'lhan ', 'lirin', 'maben', \n",
    "              'maib ', 'marri ', 'mik ', 'nale ', 'nazh', 'nergaoul', 'nbissiquan', \n",
    "              'nlent', 'noura', ' odium', 'redin', 'rez ', 'rial', 'rushu',\n",
    "              'sebarial', 'shalash', 'sidin', 'sja-anat', 'tag ', 'taka ',\n",
    "              'talik', 'temoo', 'thresh ', 'tigzikk', 'toravi', 'vamah', \n",
    "              'vao', 'vath ', 'vathah', ' veil ', 'elhokar', ' evi ', \n",
    "              'evinor']\n",
    "characters = [character.title() for character in characters] #oops title case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created a dictionary of POV section : characters within. This is used shortly after in creating the co-occurrence matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#--> iterate through each and store in dictionary\n",
    "sections_dictionary = {}\n",
    "iterative = 0\n",
    "for section in cleaned_sections:\n",
    "    iterative += 1\n",
    "    for char in characters:\n",
    "        if char in section:\n",
    "            if str(iterative) in sections_dictionary.keys():\n",
    "                sections_dictionary[str(iterative)].append(char)  \n",
    "            else:\n",
    "                sections_dictionary[str(iterative)] = [char]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this, I create  the matrix discussed above.. It's more or less a sparse co-occurrence matrix. Each row/col relates to a character, where they line up is their co-occurrence count.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##set base df (co-occurance matrix)\n",
    "df = pd.DataFrame(columns = characters, index = characters)\n",
    "df[:] = int(0)\n",
    "\n",
    "#iterate through each POV of book and add one for each character-character relationship\n",
    "#-> in this case, relationship equates to appearing in the same POV\n",
    "for value in sections_dictionary.values():\n",
    "    for character1 in characters:\n",
    "        for character2 in characters:\n",
    "            if character1 in value and character2 in value:\n",
    "                df[character1][character2] += 1\n",
    "                df[character2][character1] += 1\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this matrix, I am able to create two lists- One for the cedges for the diagram represented in a tuple of form: (char1, char2, co-occurrence weight).  I decided to make each weight a decimal in relation to the max occurence  seen: Which was Dalinar's count at 464.\n",
    "\n",
    "I also create the node list below by, more or less, the same method. This is a duple though containing: (char, total occurrences). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "              \n",
    "#add weights to edges\n",
    "edge_list = [] #test networkx\n",
    "for index, row in df.iterrows():\n",
    "    i = 0\n",
    "    for col in row:\n",
    "        weight = float(col)/464\n",
    "        edge_list.append((index, df.columns[i], weight))\n",
    "        i += 1\n",
    "\n",
    "#Remove edge if 0.0\n",
    "updated_edge_list = [x for x in edge_list if not x[2] == 0.0]\n",
    "\n",
    "#create duple of char, occurance in novel\n",
    "node_list = []\n",
    "for i in characters:\n",
    "    for e in updated_edge_list:\n",
    "        if i == e[0] and i == e[1]:\n",
    "           node_list.append((i, e[2]*6))\n",
    "for i in node_list:\n",
    "    if i[1] == 0.0:\n",
    "        node_list.remove(i)\n",
    "\n",
    "#remove self references\n",
    "for i in updated_edge_list:\n",
    "    if i[0] == i[1]:\n",
    "        updated_edge_list.remove(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that was left was to start the networkX work. I begin by setting the plot size.  Continuing I iterate through the node_list and add each one to the graph.  I then do the same for the edges, specifying the Char-Char relationship, including the line weight. \n",
    "\n",
    "I quickly ran into trouble here and learned that, to set a unique node size and edge weight for each unique point, I had to also have a size_list and width_list, respectively. I end up calling these in the draw phase. \n",
    "\n",
    "The main take-away: I had to manually print out the order of the nodes, then create a list in this order of their appropriate node sizes. Again so for the edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set canvas size\n",
    "plt.subplots(figsize=(14,14))\n",
    "\n",
    "#networkx graph time!\n",
    "G = nx.Graph()\n",
    "for i in sorted(node_list):\n",
    "    G.add_node(i[0], size = i[1])\n",
    "G.add_weighted_edges_from(updated_edge_list)\n",
    "\n",
    "#check data of graphs\n",
    "#G.nodes(data=True)\n",
    "#G.edges(data = True)\n",
    "\n",
    "#manually copy and pasted the node order using 'nx.nodes(G)'\n",
    "#Couldn't determine another route to listing out the order of nodes for future work\n",
    "node_order = ['Skar ', ' Syl ', 'Rushu', 'Kaza ', 'Peet ', 'Roshone ', 'Dabbid ',\n",
    "              'Toravi', 'Natam ', 'Adolin ', 'Shallan ', 'Navani ', 'Nightwatcher ', \n",
    "              'Gavilar ', 'Rlain ', ' Odium', 'Khal ', 'Ellista ', 'Lirin', 'Leyten ',\n",
    "              'Laral ', 'Torol ', 'Shalash', 'Inadara', 'Sigzil ', 'Elhokar', 'Venli ', \n",
    "              'Sidin', 'Wyndle ', 'Rysn ', 'Mem ', 'Palona ', 'Wit ', 'Vamah', 'Eshonai ', \n",
    "              'Lift ', 'Stormfather ', ' Evi ', 'Moash ', 'Shen ', 'Kaladin ', 'Lopen ', \n",
    "              'Szeth ', 'Renarin ', 'Taravangian ', 'Kadash', 'Nale ', 'Drehy ', 'Dukar ', \n",
    "              'Gaz ', 'Teleb ', 'Helaran ', 'Sheler ', 'Sebarial', 'Hoid ', 'Meridas ', \n",
    "              ' Pattern ', ' Timbre ', 'Kalami', 'Glys ', 'Yake ', ' Veil ', 'Nergaoul', \n",
    "              'Noura', 'Hobber ', ' Ivory ', 'Maben', 'Torfin ', 'Rial', 'Teft ', 'Dalinar ', \n",
    "              'Vathah', 'Jakamav ', 'Jasnah ', ' Rock ']\n",
    "\n",
    "#reorder node list\n",
    "updated_node_order = []\n",
    "for i in node_order:\n",
    "    for x in node_list:\n",
    "        if x[0] == i:\n",
    "            updated_node_order.append(x)\n",
    "            \n",
    "#reorder edge list - this was a pain\n",
    "test = nx.get_edge_attributes(G, 'weight')\n",
    "updated_again_edges = []\n",
    "for i in nx.edges(G):\n",
    "    for x in test.iterkeys():\n",
    "        if i[0] == x[0] and i[1] == x[1]:\n",
    "            updated_again_edges.append(test[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the sizes and edges in correct order, all that was left was to draw out the graph. This step required a lot of trial and error. The original sizes and weights were wayyyy too small, so I added scalars to explore possibilities with. The values below were the result of the trial runs. \n",
    "\n",
    "Similarlly, The default graph was inappropriate for such a large network. I played around with the splring_layout(K, iterations) params to find a happy-medium between edge distance and iterations of graphing. These two values below result (typically) in a nice looking network. It's randomized so the visual will display a different version each time it is ran.  The nodes and edge sizes remain the same, yet the locations of the nodes within the graph will be randomly placed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#drawing custimization\n",
    "node_scalar = 800\n",
    "edge_scalar = 10\n",
    "sizes = [x[1]*node_scalar for x in updated_node_order]\n",
    "widths = [x*edge_scalar for x in updated_again_edges]\n",
    "\n",
    "#draw the graph\n",
    "pos = nx.spring_layout(G, k=0.42, iterations=17)\n",
    "\n",
    "nx.draw(G, pos, with_labels=True, font_size = 8, font_weight = 'bold', \n",
    "        node_size = sizes, width = widths)\n",
    "\n",
    "#plt.axis('off')\n",
    "#plt.savefig(\"imgs/sl_network2.png\") # save as png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, I'm very excited about how this turned out! I got some great experience creating network diagrams with a fun python package, and was able to visualize how the characters within Oathbringer connect. \n",
    "\n",
    "The final visual posted above is the result of this code combined with some title creation in Paint.net. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
